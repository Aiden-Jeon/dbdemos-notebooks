# Databricks notebook source
# MAGIC %md-sandbox
# MAGIC
# MAGIC # 1/ Data preparation for LLM Chatbot RAG
# MAGIC
# MAGIC ## Building our knowledge base and preparing our documents for Databricks Vector Search
# MAGIC
# MAGIC <img src="https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep.png?raw=true" style="float: right; width: 800px; margin-left: 10px">
# MAGIC
# MAGIC In this notebook, we'll prepare data for our Vector Search Index.
# MAGIC
# MAGIC Preparing high quality data is key for your chatbot performance. We recommend taking time implementing this with your own dataset.
# MAGIC
# MAGIC For this example, we will use Databricks documentation from [docs.databricks.com](docs.databricks.com):
# MAGIC - Download the web pages
# MAGIC - Split the pages in small chunks
# MAGIC - Extract the text from the HTML content
# MAGIC
# MAGIC Thankfully, Lakehouse AI not only provides state of the art solutions to accelerate your AI and LLM projects, but also to accelerate data ingestion and preparation at scale.
# MAGIC
# MAGIC *Note: While some processing in this notebook is specific to our dataset (exmple: splitting chunks around `h2` elements), **we strongly recommend getting familiar with the overall process and replicate that on your own dataset**.*

# COMMAND ----------

# DBTITLE 1,Install required external libraries 
# installing a Python library to help us extract data out of HTML and XML files
# installing a tokenizer library 
%pip install beautifulsoup4==4.11.1 transformers

# COMMAND ----------

# MAGIC %run ./_resources/00-init $catalog=dbdemos $db=chatbot_tuning $reset_all_data=false

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC ## Extracting Databricks documentation sitemap and pages
# MAGIC
# MAGIC <img src="https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-data-prep-1.png?raw=true" style="float: right; width: 600px; margin-left: 10px">
# MAGIC
# MAGIC First, let's create our raw dataset as a Delta table.
# MAGIC
# MAGIC For this demo, we will directly download a few documentation pages from `docs.databricks.com`  and save the HTML content.
# MAGIC
# MAGIC Here are the main steps:
# MAGIC
# MAGIC - Run a quick script to extract the page URLs from the `sitemap.xml` file
# MAGIC - Download the web pages
# MAGIC - Use BeautifulSoup to extract the ArticleBody
# MAGIC - Save the result in a Delta Lake table
# MAGIC
# MAGIC *Note: for faster execution time, we will only download ~100 pages. Make sure you use these pages to ask questions to your model and see RAG in action!*

# COMMAND ----------

# MAGIC %md
# MAGIC ## Knowledge base dataset used during RAG
# MAGIC
# MAGIC The first step for us is to start creating the Knowledge Datasets. In our example, we used the documentation pages from docs.databricks.com (mid-2023 snapshot).
# MAGIC
# MAGIC ### Dataset chunk size
# MAGIC Note that this dataset was already cleaned up, and split into small chunks so that they can fit the context window size and not be too expensive.
# MAGIC
# MAGIC The standard way is to split long texts into smaller paragraph chunks (for example between h2 blocks on HTML pages). The chunk size is typically computed using your LLM tokenizer (in our case llama2) 
# MAGIC
# MAGIC This part is critical for your chatbot performance. We won't go into the details how we built our documentation dataset in this demo, this is covered in the `dbdemos.instal('llm-rag-chatbot')` demo (we strongly recommend to start here).

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS databricks_documentation  (id BIGINT GENERATED BY DEFAULT AS IDENTITY, url STRING, content STRING, title STRING);
# MAGIC ALTER TABLE databricks_documentation SET OWNER TO `account users`;

# COMMAND ----------

# MAGIC %sql
# MAGIC COPY INTO databricks_documentation
# MAGIC   FROM '/dbdemos/product/llm/databricks-doc/databricks_documentation.parquet'
# MAGIC   FILEFORMAT = PARQUET  COPY_OPTIONS ('mergeSchema' = 'true');
# MAGIC   
# MAGIC SELECT * FROM databricks_documentation;

# COMMAND ----------

# MAGIC %md
# MAGIC ## Generating our fine tuning dataset
# MAGIC
# MAGIC To train our LLM, we need to have an ensemble of Documents, Questions and Answers.
# MAGIC
# MAGIC - Documents + Questions: Question that are relevant to a specific question. We will use this to train embeddings with our LLM
# MAGIC - Questions + Answers to these questions: We will use this to fine tune our main completion LLM. 
# MAGIC
# MAGIC In a real-world situation, this information would typically be gathered by scanning your customers question in an existing chatbot of support system, and then have an expert answering these questions pointing the link to the relevant documentation page. 
# MAGIC
# MAGIC For this demo, we used Databricks SQL AI function to generate the question and the answer.
# MAGIC
# MAGIC For each document within our Documentation table, we asked an external LLM to ask a question about the documentation page, and then answer the question.<br/>
# MAGIC For more details on how to do this, check the [./_resources/01-Data-Preparation-full]($././_resources/01-Data-Preparation-full) or install the `dbdemos.install('sql-ai-functions')` demo.
# MAGIC
# MAGIC
# MAGIC *Remember that the underlying LLM licence applies when you use this dataset to fine-tune your own model. The model we will be training can't be used for commercial purpose, and this demo is for educational purpose only.*

# COMMAND ----------

# MAGIC %md 
# MAGIC ## Loading our Document Questions
# MAGIC
# MAGIC We will use this information to fine-tune our model embeddings and find more relevant documents related to the user question and send them as RAG context.

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS training_dataset_question;
# MAGIC COPY INTO training_dataset_question
# MAGIC   FROM '/dbdemos/product/llm/databricks-doc/training_dataset_question.parquet'
# MAGIC   FILEFORMAT = PARQUET   COPY_OPTIONS ('mergeSchema' = 'true');
# MAGIC
# MAGIC SELECT * FROM training_dataset_question;

# COMMAND ----------

# MAGIC %md 
# MAGIC ## Loading our Answers
# MAGIC
# MAGIC Let's get the question answers. We will use the Question + Answer dataset together to train our autocompletion LLM.

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS training_dataset_answer;
# MAGIC COPY INTO training_dataset_answer
# MAGIC   FROM '/dbdemos/product/llm/databricks-doc/training_dataset_answer.parquet'
# MAGIC   FILEFORMAT = PARQUET   COPY_OPTIONS ('mergeSchema' = 'true');
# MAGIC SELECT * FROM training_dataset_answer;

# COMMAND ----------

# MAGIC %md 
# MAGIC # Joining everything together
# MAGIC

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM databricks_documentation d
# MAGIC   INNER JOIN training_dataset_question q on q.doc_id = d.id
# MAGIC   INNER JOIN training_dataset_answer   a on a.question_id = q.id

# COMMAND ----------

# MAGIC %md 
# MAGIC Let's now split our entire dataset using this function using a pandas UDF.
# MAGIC
# MAGIC We will also extract the title from the page (based on the `h1` tag)

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## Our dataset is now ready! Let's start deploying our first model for embeddings
# MAGIC
# MAGIC Our dataset is now ready, and saved as a Delta Lake table.
# MAGIC
# MAGIC Building a clean dataset is key for the next operations. Make sure your pipeline is clean and generate high quality Q&A.
# MAGIC
# MAGIC This is where the Lakehouse will accelerate your journey: by leveraging DLT, expectations and UC data quality monitoring your easily ingest and prepare unstructured data for your ML training.
# MAGIC
# MAGIC Remember, this is the real power of the Lakehouse: one unified platform for data preparation, analysis and AI.
# MAGIC
# MAGIC Next: Open the [02-Creating-Vector-Index]($./02-Creating-Vector-Index) notebook and create our embedding endpoint and index.
